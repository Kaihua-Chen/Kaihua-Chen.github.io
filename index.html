<!DOCTYPE HTML>
<html lang="en">
  <head>
    <meta http-equiv="Content-Type" content="text/html; charset=UTF-8">

    <title>Kaihua Chen</title>

    <meta name="author" content="Kaihua Chen">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <link rel="shortcut icon" href="images/kaihuac.jpeg" type="image/jpeg">
    <link rel="stylesheet" type="text/css" href="stylesheet.css">

  </head>

  <body>
    <table style="width:100%;max-width:800px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
      <tr style="padding:0px">
        <td style="padding:0px">
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr style="padding:0px">
              <td style="padding:2.5%;width:63%;vertical-align:middle">
                <p class="name" style="text-align: center;">
                  Kaihua Chen
                </p>
                <p>
		I’m a Master’s student in Computer Vision (<a href="https://www.ri.cmu.edu/education/academic-programs/master-of-science-computer-vision/">MSCV</a>) at the 
    <a href="https://www.ri.cmu.edu/">Robotics Institute</a>, <a href="https://www.cs.cmu.edu/">School of Computer Science</a>, <a href="https://www.cmu.edu/">Carnegie Mellon University</a>, 
    advised by <a href="https://www.cs.cmu.edu/~deva/">Prof. Deva Ramanan</a>. 
     My research broadly focuses on diffusion generative models and 3D/4D vision.
     Previously, I was fortunate to work as a research intern at the <a href="https://www.utoronto.ca/">University of Toronto</a> during my undergraduate studies.
                </p>
                <p style="text-align:center">
                  <a href="mailto:kaihuac5@gmail.com">Email</a> &nbsp;/&nbsp;
                  <a href="data/Kaihua_Chen_Resume.pdf">CV</a> &nbsp;/&nbsp;
                  <a href="https://scholar.google.com/citations?hl=en&user=grE9XU0AAAAJ">Scholar</a> &nbsp;/&nbsp;
                  <a href="https://github.com/Kaihua-Chen">Github</a>
                </p>
              </td>
              <td style="padding:2.5%;width:37%;max-width:37%">
                <a href="images/kaihuac.jpeg"><img style="width:60%;max-width:60%;object-fit: cover;" alt="profile photo" src="images/kaihuac.jpeg" class="hoverZoomLink"></a>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
              <tr>
              <td style="padding:16px;width:100%;vertical-align:middle">
                <h2>Research</h2>
                <p>
                  My current research focuses on leveraging diffusion priors, which capture both photorealistic generation and underlying 3D structure,
                   for tasks including amodal segmentation, video understanding, depth estimation, and 4D reconstruction.
                </p>
              </td>
            </tr>
          </tbody></table>
          <table style="width:100%;border:0px;border-spacing:0px 10px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>


    <tr>
      <td style="padding:4px 16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="one" id='cognvs_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/cognvs.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://cog-nvs.github.io/">
          <span class="papertitle">Reconstruct, Inpaint, Finetune: Dynamic Novel-view Synthesis from Monocular Videos</span>
        </a>
        <br>
        <strong>Kaihua Chen <sup>*</sup> </strong>,
        <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana <sup>*</sup></a>,
        <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>
        <br>
        In submission
        <br>
        <a href="https://cog-nvs.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2507.12646">arXiv</a>
        /
        <a href="https://github.com/Kaihua-Chen/cog-nvs">code (coming soon)</a>
        <p>
          We reformulate novel-view synthesis as a structured inpainting task.
          CogNVS is a video diffusion model for dynamic novel-view synthesis trained in a self-supervised manner using only 2D videos! 
        </p>
      </td>
    </tr>

    <tr>
      <td style="padding:4px 16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="one" id='diffusion_vas_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/diffusion-vas.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://diffusion-vas.github.io/">
          <span class="papertitle">Using Diffusion Priors for Video Amodal Segmentation</span>
        </a>
        <br>
        <strong>Kaihua Chen</strong> ,
        <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
        <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>
        <br>
        <em>CVPR</em>, 2025
        <br>
        <a href="https://diffusion-vas.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2412.04623">arXiv</a>
        /
        <a href="https://github.com/Kaihua-Chen/diffusion-vas">code</a>
        <p>
          Given a modal (visible) object sequence in a video, 
          we develop a two-stage method that generates its amodal (visible + invisible) masks and RGB content via video diffusion.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:4px 16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="one" id='mfh_image'>
        <img src='images/mfh.png' width="100%" height="100%"></img>
      </div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://neurips.cc/virtual/2024/poster/95921">
          <span class="papertitle">Metric from Human: Zero-shot Monocular Metric Depth Estimation via Test-time Adaptation</span>
        </a>
        <br>
        <a href="https://scholar.google.com/citations?user=nVKRaf4AAAAJ&hl=en">Yizhou Zhao</a>,
        <a href="https://bianhengwei.com/">Hengwei Bian</a>,
        <strong>Kaihua Chen</strong> ,
        <a href="https://scholar.google.com/citations?user=NEAhh3IAAAAJ&hl=en">Pengliang Ji</a>,
        <a href="https://scholar.google.com/citations?user=IDbqDdEAAAAJ&hl=zh-CN">Liao Qu</a>,
        <a href="https://openreview.net/profile?id=~Shao-yu_Lin1">Shao-yu Lin</a>,
        <a href="https://weichen-yu.github.io/">Weichen Yu</a>,
        <a href="https://haoranli525.github.io/">Haoran Li</a>,
        <a href="https://hhhhhhao.github.io/">Hao Chen</a>,
        <a href="https://scholars.uow.edu.au/jun-shen">Jun Shen</a>,
        <a href="https://scholar.google.com/citations?user=IWcGY98AAAAJ&hl=en">Bhiksha Raj</a>,
        <a href="https://scholar.google.com/citations?user=Y3Cqt0cAAAAJ&hl=en">Min Xu</a>,
        <br>
        <em>NeurIPS</em>, 2024
        <br>
        <a href="https://neurips.cc/virtual/2024/poster/95921">project page</a>
        /
        <a href="https://neurips.cc/virtual/2024/poster/95921">paper</a>
        /
        <a href="https://github.com/Skaldak/MfH">code</a>
        <p>
          MfH converts relative depth estimation to metric depth estimation via generative painting and human mesh recovery.
        </p>
      </td>
    </tr>


    <tr>
      <td style="padding:4px 16px;width:20%;vertical-align:middle">
        <div class="one">
          <div class="one" id='tao_amodal_image'><video  width=100% height=100% muted autoplay loop>
          <source src="images/tao_amodal.mp4" type="video/mp4">
          Your browser does not support the video tag.
          </video></div>
        </div>
      </td>
      <td style="padding:8px;width:80%;vertical-align:middle">
        <a href="https://tao-amodal.github.io/">
          <span class="papertitle">TAO-Amodal: A Benchmark for Tracking Any Object Amodally</span>
        </a>
        <br>
        <a href="https://wesleyhsieh0806.github.io/">Cheng-Yen (Wesley) Hsieh</a>,
        <strong>Kaihua Chen</strong> ,
        <a href="https://www.achaldave.com/">Achal Dave</a>
        <a href="https://www.cs.cmu.edu/~tkhurana/">Tarasha Khurana</a>
        <a href="https://www.cs.cmu.edu/~deva/">Deva Ramanan</a>,
        <br>
        <em>arXiv</em>, 2024
        <br>
        <a href="https://tao-amodal.github.io/">project page</a>
        /
        <a href="https://arxiv.org/abs/2312.12433">arXiv</a>
        /
        <a href="https://github.com/WesleyHsieh0806/TAO-Amodal">code</a>
        /
        <a href="https://huggingface.co/datasets/chengyenhsieh/TAO-Amodal">dataset</a>
        <p>
          We introduce TAO-Amodal, an amodal tracking dataset
          featuring 833 diverse categories in thousands of video sequences.
        </p>
      </td>
    </tr>
    

    

    </tbody></table>
          
          <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
            <tr>
              <td style="padding:0px">
                <br>
                <p style="text-align:right;font-size:small;">
                  Template borrowed from <a href="https://github.com/jonbarron/jonbarron_website">Jon Barron</a>. <br>
                  Last updated: July 23, 2025
                </p>
              </td>
            </tr>
          </tbody></table>
        </td>
      </tr>
    </table>
  </body>
</html>
